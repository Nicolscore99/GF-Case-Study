{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9779e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pandasql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad6715d",
   "metadata": {},
   "source": [
    "# Data Analysis for GOS Case #1 with Georg Fischer AG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fcbd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from pandasql import sqldf\n",
    "import itertools\n",
    "from itertools import product\n",
    "import requests\n",
    "import json\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7370ac9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete_dataset_cost_breakdown = pd.read_csv('cost_breakdown.csv', \n",
    "                                                 delimiter=';', \n",
    "                                                 encoding= 'unicode_escape', \n",
    "                                                 on_bad_lines='skip')\n",
    "df_complete_dataset_market_view = pd.read_csv('market_view.csv', \n",
    "                                              delimiter=';', \n",
    "                                              encoding= 'unicode_escape', \n",
    "                                              on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80f118a",
   "metadata": {},
   "source": [
    "## 1) Sales Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04888b01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Prediction V3:\n",
    "\n",
    "# Sum Sales 2022 and put into a column:\n",
    "Quantity_Columns_2022 = ['Invoiced Quant. (BU)', 'Invoiced Quant. (BU).1', 'Invoiced Quant. (BU).2',\n",
    "                      'Invoiced Quant. (BU).3', 'Invoiced Quant. (BU).4', 'Invoiced Quant. (BU).5',\n",
    "                      'Invoiced Quant. (BU).6', 'Invoiced Quant. (BU).7', 'Invoiced Quant. (BU).8',\n",
    "                      'Invoiced Quant. (BU).9', 'Invoiced Quant. (BU).10', 'Invoiced Quant. (BU).11']\n",
    "\n",
    "Net_Value_Columns_2022 = ['Net Value (GC)', 'Net Value (GC).1', 'Net Value (GC).2',\n",
    "                          'Net Value (GC).3', 'Net Value (GC).4', 'Net Value (GC).5',\n",
    "                          'Net Value (GC).6', 'Net Value (GC).7', 'Net Value (GC).8',\n",
    "                          'Net Value (GC).9', 'Net Value (GC).10', 'Net Value (GC).11']\n",
    "# print(df_complete_dataset_market_view[Net_Value_Columns_2022])\n",
    "\n",
    "# Replace 'PC' character with empty string for Unites\n",
    "df_complete_dataset_market_view[Quantity_Columns_2022] = (\n",
    "    df_complete_dataset_market_view[Net_Value_Columns_2022].replace(\"PC\", \"\", regex=True))\n",
    "\n",
    "# Replace ' character with empty string for Sales\n",
    "df_complete_dataset_market_view[Net_Value_Columns_2022] = (\n",
    "    df_complete_dataset_market_view[Net_Value_Columns_2022].replace(\"'\", \"\", regex=True))\n",
    "\n",
    "# Convert sales columns to numeric values\n",
    "df_complete_dataset_market_view[Net_Value_Columns_2022] = (\n",
    "    df_complete_dataset_market_view[Net_Value_Columns_2022].apply(pd.to_numeric, errors='coerce'))\n",
    "\n",
    "# Summing up the monthly sales to total yearly sales\n",
    "df_complete_dataset_market_view['Total_Sales_2022'] = (\n",
    "    df_complete_dataset_market_view[Net_Value_Columns_2022].sum(axis=1, skipna=True))\n",
    "df_complete_dataset_market_view['Total_Unites_2022'] = (\n",
    "    df_complete_dataset_market_view[Quantity_Columns_2022].sum(axis=1, skipna=True))\n",
    "\n",
    "# Add columns for predicted sales from 2022 to 2031\n",
    "for year in range(2022, 2032):\n",
    "    col_name = f'Predicted Sales {year}'\n",
    "    growth_rate = df_complete_dataset_market_view['Growth rate untill 2031']  # f'ing typo\n",
    "    total_sales = df_complete_dataset_market_view['Total_Sales_2022']\n",
    "    growth_rate_numeric = (abs(pd.to_numeric(growth_rate.str.strip('%'))))**(0.1) / 100\n",
    "    predicted_sales = total_sales * (1 + growth_rate_numeric) ** (year - 2022)\n",
    "    df_complete_dataset_market_view[col_name] = predicted_sales\n",
    "\n",
    "df_complete_dataset_market_view.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2566cce9",
   "metadata": {},
   "source": [
    "## 2) CoG Light"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759f4378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input:  Dataframe containing the destinations and the weighting, everything already needs to be added up per \n",
    "#         country, function does not necessarily need to contain the latitude and longitude since I will take this \n",
    "#         from the relevant countries list\n",
    "# Output: Weighted gravity\n",
    "\n",
    "def CoG_light_overhead_sales(df_all_deliveries, str_name_col_dest, str_name_col_weight):\n",
    "    \n",
    "    weighted_sum_lat = 0.0\n",
    "    weighted_sum_lon = 0.0\n",
    "    sum_weight_total = 0.0\n",
    "    \n",
    "    ## Analysis for total sales\n",
    "    #for row in df_all_deliveries.iterrows():\n",
    "    #     if row[1][str_name_col_dest] in rel_country_lst:\n",
    "    #        sum_weight_total += row[1][str_name_col_weight]\n",
    "    #        weighted_sum_lon += relevant_countries.loc[relevant_countries['Country'] == (\n",
    "    # row[1][str_name_col_dest],'Longitude'].tolist()[0]*(row[1][str_name_col_weight]))\n",
    "    #        weighted_sum_lat += relevant_countries.loc[relevant_countries['Country'] == (\n",
    "    # row[1][str_name_col_dest],'Latitude'].tolist()[0]*(row[1][str_name_col_weight]))\n",
    "    \n",
    "    ## Analysis for just the overhead\n",
    "    for row in df_all_deliveries.iterrows():\n",
    "            overhead = row[1][str_name_col_weight]-row[1]['Total_Sales_2022']\n",
    "            sum_weight_total += overhead\n",
    "            weighted_sum_lon += relevant_countries.loc[relevant_countries['Country'] == (\n",
    "                row[1][str_name_col_dest],'Longitude'].tolist()[0]*overhead)\n",
    "            weighted_sum_lat += relevant_countries.loc[relevant_countries['Country'] == (\n",
    "                row[1][str_name_col_dest],'Latitude'].tolist()[0]*overhead)\n",
    "    \n",
    "    return weighted_sum_lat/sum_weight_total, weighted_sum_lon/sum_weight_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43a083a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input:  Dataframe containing the destinations and the weighting, everything already needs to be added up per \n",
    "#         country, function does not necessarily need to contain the latitude and longitude since I will take this \n",
    "#         from the relevant countries list\n",
    "# Output: Weighted gravity\n",
    "\n",
    "def CoG_light_total_sales(df_all_deliveries, str_name_col_dest, str_name_col_weight):\n",
    "    \n",
    "    weighted_sum_lat = 0.0\n",
    "    weighted_sum_lon = 0.0\n",
    "    sum_weight_total = 0.0\n",
    "    \n",
    "    ## Analysis for total sales\n",
    "    for row in df_all_deliveries.iterrows():\n",
    "            sum_weight_total += row[1][str_name_col_weight]\n",
    "            weighted_sum_lon += (\n",
    "            relevant_countries.loc[relevant_countries['Country'] == row[1][str_name_col_dest], \n",
    "                                   'Longitude'].tolist()[0]\n",
    "                                    * (row[1][str_name_col_weight]))\n",
    "\n",
    "            weighted_sum_lat += (relevant_countries.loc[relevant_countries['Country'] == row[1][str_name_col_dest],\n",
    "                                    'Latitude'].tolist()[0]\n",
    "                                    * (row[1][str_name_col_weight]))\n",
    "\n",
    "\n",
    "    return weighted_sum_lat/sum_weight_total, weighted_sum_lon/sum_weight_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ebd5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_countries = pd.read_csv('relevant_countries.txt', \n",
    "                                 delimiter=',', \n",
    "                                 encoding= 'unicode_escape', \n",
    "                                 on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbf7f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a dataframe that only includes the relevant destinations\n",
    "\n",
    "relevant_orders = df_complete_dataset_market_view.loc[\n",
    "    df_complete_dataset_market_view['Country Ship-To'].isin(\n",
    "        relevant_countries['Country'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03aca96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an array of all the data you want to look at\n",
    "\n",
    "weighting_parameter = [\n",
    "    'Predicted Sales 2023',\n",
    "    'Predicted Sales 2024',\n",
    "    'Predicted Sales 2025',\n",
    "    'Predicted Sales 2026',\n",
    "    'Predicted Sales 2027',\n",
    "    'Predicted Sales 2028',\n",
    "    'Predicted Sales 2029',\n",
    "    'Predicted Sales 2030',\n",
    "    'Predicted Sales 2031',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bd47bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_coordinates = np.zeros((len(weighting_parameter), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4003d2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, yearly_weighting_param in enumerate(weighting_parameter):\n",
    "    lat_i, lon_i = CoG_light_total_sales(relevant_orders, \"Country Ship-To\", yearly_weighting_param)\n",
    "    optimal_coordinates[i][0] = lat_i\n",
    "    optimal_coordinates[i][1] = lon_i\n",
    "    print(lat_i, lon_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53708079",
   "metadata": {},
   "outputs": [],
   "source": [
    "CoG_per_year = pd.DataFrame()\n",
    "CoG_per_year['year'] = weighting_parameter\n",
    "CoG_per_year['latitude'] = optimal_coordinates.T[0]\n",
    "CoG_per_year['longitude'] = optimal_coordinates.T[1]\n",
    "CoG_per_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2bee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CoG_per_year.to_csv('Simple_CoG.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b983eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_mapbox(\n",
    "    CoG_per_year.tail(1),\n",
    "    lat=\"latitude\",\n",
    "    lon=\"longitude\",\n",
    "    hover_name=\"year\",\n",
    "    color='year',\n",
    "    size=[10],\n",
    "    zoom=3,\n",
    "    height=300\n",
    ")\n",
    "fig.update_layout(mapbox_style=\"open-street-map\")\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa2c4ab",
   "metadata": {},
   "source": [
    "## 3) Fancy CoG Analysis\n",
    "\n",
    "### 3.1) Choose City Combinations\n",
    "- Arbitrarily chose a number of n cities that fullfill the following requrements\n",
    "    - located in eastern europe\n",
    "    - population size > 5000\n",
    "- Make the combinations with the destination countries in eastern europe\n",
    "\n",
    "### 3.2) Calculate distance\n",
    "- Using the OSRM API, calculate the distance between all these combinations\n",
    "\n",
    "### 3.3) Find the weighing factor for each destination location\n",
    "- Assign a volume coefficient and a material coefficient\n",
    "    - Volume coefficient: in range[1, ..., 3] depending on the volume of the good\n",
    "    - Material coefficient: in range[1, ..., 3] depending on the weight of the good\n",
    "- Itherate through the market_view dataset and calculate the weighing factor for each order\n",
    "- Weighing factor:\n",
    "\n",
    "        $Factor = #Pieces*Material_Coefficient*Volume_Coefficient$\n",
    "\n",
    "- We calculate this for the overhead of the production compared to current production rate for each year.\n",
    "- As we will need the weighing factor per destination, we then summarize the weighing factor for all orders that go to a certain destination.\n",
    "\n",
    "### 3.4) Make a minimization analysis\n",
    "- Here we simply multiply the travelling distance calculated in step 2 with the weighing factor.\n",
    "- The best result will then be the one with the heat map\n",
    "\n",
    "### 3.5) Map the changes\n",
    "- These results would look absolutely stunning on a heat map\n",
    "- We can also map everything from best to worst places and how the projections would look like over the years\n",
    "- https://predictivehacks.com/how-to-create-heatmap-on-a-map-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8057fd50",
   "metadata": {},
   "source": [
    "## 3.1) Choose city combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5be906",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_european_cities = pd.read_csv(\n",
    "    'geonames-all-european-cities.csv',\n",
    "    delimiter=';',\n",
    "    encoding='unicode_escape',\n",
    "    on_bad_lines='skip'\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b2deb890",
   "metadata": {},
   "source": [
    "all_european_cities.head(3)\n",
    "print(len(all_european_cities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0eaa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# City should have > 5k inhabitants and be contained in the relevant countries list\n",
    "reduced_city_list = all_european_cities[\n",
    "    (all_european_cities['Population'] > 5000) &\n",
    "    (all_european_cities['Country name'].isin(relevant_countries['Country']))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18049e46",
   "metadata": {},
   "source": [
    "## 3.2) Calculate the distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31ed0a2",
   "metadata": {},
   "source": [
    "## !!! Attention !!! \n",
    "Reduced_city_list still needs to be samples. Otherwise the model explodes. Sampling 20/3721 cities in eastern europe leads to 420 combinations which takes the distance calculation API ~3min to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745789fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cities = pd.DataFrame()\n",
    "num_cities_per_country = 20\n",
    "for row in relevant_countries['Country']:\n",
    "    temp_df = reduced_city_list[reduced_city_list['Country name'].isin([row])]\n",
    "    \n",
    "    if(len(temp_df)>num_cities_per_country):\n",
    "        selected_cities = pd.concat([selected_cities, temp_df.sample(num_cities_per_country)])\n",
    "    else:\n",
    "        selected_cities = pd.concat([selected_cities, temp_df])\n",
    "    \n",
    "print(len(selected_cities))\n",
    "print(selected_cities.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b74a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique permutations for driving distances\n",
    "# Todo: Do we really want to consider countries within the same region?\n",
    "\n",
    "list1 = relevant_countries['Capital'].tolist()\n",
    "list2 = selected_cities['Name'].tolist()\n",
    "\n",
    "city_pairs = list(itertools.product(list1, list2))\n",
    "print(len(city_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b106847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_mapbox(selected_cities, \n",
    "                        lat=\"Latitude\", \n",
    "                        lon=\"Longitude\", \n",
    "                        hover_name=\"Name\", \n",
    "                        color='Country name', \n",
    "                        zoom=3, \n",
    "                        height=300)\n",
    "fig.update_layout(mapbox_style=\"open-street-map\")\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa08d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city_combinations = pd.DataFrame()\n",
    "df_city_combinations[\"Potential Factory Placement\"] = np.array(city_pairs).T[1]\n",
    "df_city_combinations[\"Destination\"] = np.array(city_pairs).T[0]\n",
    "df_city_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59022e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the distance between all these combinations and append to the dataframe\n",
    "\n",
    "dist_lst = []\n",
    "i = 0\n",
    "\n",
    "for row in df_city_combinations.iterrows():\n",
    "    lon_1 = relevant_countries.loc[relevant_countries['Capital'] == row[1]['Destination'],'Longitude'].tolist()[0]\n",
    "    lat_1 = relevant_countries.loc[relevant_countries['Capital'] == row[1]['Destination'],'Latitude'].tolist()[0]\n",
    "    lon_2 = (\n",
    "    selected_cities.loc[\n",
    "        selected_cities['Name'] == row[1]['Potential Factory Placement'],\n",
    "        'Longitude'].tolist()[0])\n",
    "    lat_2 = (\n",
    "    selected_cities.loc[\n",
    "        selected_cities['Name'] == row[1]['Potential Factory Placement'],\n",
    "        'Latitude'].tolist()[0])\n",
    "    r = requests.get(f\"http://router.project-osrm.org/route/v1/car/{lon_1},{lat_1};{lon_2},{lat_2}?overview=false\"\"\")\n",
    "    routes = json.loads(r.content)\n",
    "    best_route = routes.get(\"routes\")[0]\n",
    "    dist_lst.append(best_route[\"distance\"])\n",
    "    i += 1\n",
    "    print(i, \" of \", len(df_city_combinations))\n",
    "    \n",
    "df_city_combinations['Distance'] = dist_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9010f079",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city_combinations['Distance'] = dist_lst\n",
    "avg_dist = np.average(df_city_combinations['Distance'])\n",
    "df_city_combinations['Distance_normalized'] = [i/avg_dist for i in df_city_combinations['Distance']]\n",
    "df_city_combinations.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f926ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city_combinations.to_csv('df_city_combinations'+str(len(df_city_combinations))+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3662de",
   "metadata": {},
   "source": [
    "## 3.3) Find the weighing factor for each destination location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d1b542",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_classification = pd.read_csv('weight_classification.CSV', \n",
    "                                    delimiter=';', \n",
    "                                    encoding= 'unicode_escape', \n",
    "                                    on_bad_lines='skip')\n",
    "volume_classification = pd.read_csv('volume_classification.CSV', \n",
    "                                    delimiter=';', \n",
    "                                    encoding= 'unicode_escape', \n",
    "                                    on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47482be",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_classification['density_normalized'] = [\n",
    "    i / np.average(weight_classification['density'])\n",
    "    for i in weight_classification['density']\n",
    "]\n",
    "\n",
    "volume_classification['volume_normalized'] = [\n",
    "    i / np.average(volume_classification['vol_per_item'])\n",
    "    for i in volume_classification['vol_per_item']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e143e9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_classification.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5d9d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_classification.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118e9f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Assigne the volume and the weights to the general table for all offers\n",
    "\n",
    "lst_vol_classifcation_per_order = []\n",
    "lst_weight_classifcation_per_order = []\n",
    "\n",
    "for row in relevant_orders.iterrows():\n",
    "    try: \n",
    "        v = (\n",
    "    volume_classification.loc[\n",
    "        volume_classification['product_group'] == str(row[1]['Product Family Short']),\n",
    "        'volume_normalized'\n",
    "    ].to_list()[0]\n",
    ")\n",
    "\n",
    "    except:\n",
    "        v = 1.0\n",
    "    \n",
    "    try:\n",
    "        w = (\n",
    "    weight_classification.loc[\n",
    "        weight_classification['Material'] == row[1]['Material'],\n",
    "        'density_normalized'\n",
    "    ].to_list()[0]\n",
    ")\n",
    "\n",
    "    except:\n",
    "        w = 1.0\n",
    "    \n",
    "    lst_vol_classifcation_per_order.append(v)\n",
    "    lst_weight_classifcation_per_order.append(w)\n",
    "        \n",
    "relevant_orders['weight_classification'] = lst_weight_classifcation_per_order\n",
    "relevant_orders['volume_classification'] = lst_vol_classifcation_per_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18550f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_volume = 3.0\n",
    "weight_weight = 1.0\n",
    "\n",
    "weight_vol_norm = weight_volume/(weight_volume+weight_weight)\n",
    "weight_wei_norm = weight_weight/(weight_volume+weight_weight)\n",
    "\n",
    "def valuation_factor(sales_avg_comp, vol_avg_comp, weight_avg_comp):\n",
    "    return sales_avg_comp*(weight_vol_norm*weight_avg_comp + weight_wei_norm*vol_avg_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0968bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_sales_overhead = []\n",
    "for row in relevant_orders.iterrows():\n",
    "    sales_overhead = row[1]['Predicted Sales 2031']-row[1]['Total_Sales_2022']\n",
    "    lst_sales_overhead.append(sales_overhead)\n",
    "    \n",
    "relevant_orders['sales_overhead_2031'] = lst_sales_overhead\n",
    "\n",
    "avg_sales_overhead = np.mean(lst_sales_overhead)\n",
    "\n",
    "lst_sales_overhead_normalized = [i/avg_sales_overhead for i in lst_sales_overhead]\n",
    "relevant_orders['sales_overhead_2031_normalized'] = lst_sales_overhead_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cf5508",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_weighing_factor_lst = []\n",
    "for row in relevant_orders.iterrows():\n",
    "    _w = valuation_factor(row[1]['sales_overhead_2031_normalized'], \n",
    "                          row[1]['volume_classification'], \n",
    "                          row[1]['weight_classification'])\n",
    "    final_weighing_factor_lst.append(_w)\n",
    "    \n",
    "relevant_orders['final_weiging_factor'] = final_weighing_factor_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d470f9",
   "metadata": {},
   "source": [
    "df_score = reduced_city_list.loc[reduced_city_list['Name'].isin(df_city_combinations['Potential Factory Placement'])].drop_duplicates()\n",
    "df_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcf09de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Now do the weighting\n",
    "### We need a dataframe with all the potential cities, their coordinates\n",
    "df_score = selected_cities\n",
    "score_lst = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "for city_samples in df_score.iterrows():\n",
    "    i += 1\n",
    "    potential_location_score = 0.0\n",
    "    # take distance normalized from df_city_combinations at dest\n",
    "    \n",
    "    # for all potential cityies we are looking at\n",
    "    # for all orders\n",
    "        # take the distance inbetween the city that we are looking at and the destination of the order\n",
    "    \n",
    "    for orders in relevant_orders.iterrows():\n",
    "        capital_city = (\n",
    "    relevant_countries.loc[relevant_countries['Country'] == orders[1]['Country Ship-To'],'Capital'].values[0])\n",
    "\n",
    "        potential_location_score += (df_city_combinations.loc[\n",
    "        df_city_combinations['Destination'] == capital_city\n",
    "    ].loc[df_city_combinations['Potential Factory Placement'] == city_samples[1]['Name'],'Distance_normalized']\n",
    "    .values[0]* orders[1]['final_weiging_factor'])\n",
    "\n",
    "    \n",
    "    print(i, \" of \", len(city_samples))\n",
    "    \n",
    "    score_lst.append(potential_location_score)\n",
    "                    \n",
    "df_score['score'] = score_lst\n",
    "df_score['score_inverted'] = [1/i for i in score_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f436620",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1981935",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_score.loc[df_score['score'] == min(df_score['score'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b59411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(df_score['score']))\n",
    "print(min(df_score['score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f874df",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = df_score.sort_values(by=['score'], ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dc8af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('score.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f1357e",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = np.mean(df_score['score_inverted'])\n",
    "df_score['score_inverted_normalized'] = [i/avg for i in df_score['score_inverted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dcd639",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda046dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_mapbox(df_score, lat=\"Latitude\", lon=\"Longitude\", hover_name=\"Name\", color='score_inverted_normalized', color_continuous_scale=px.colors.cyclical.IceFire, zoom=3, height=300)\n",
    "fig.update_layout(mapbox_style=\"open-street-map\")\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
